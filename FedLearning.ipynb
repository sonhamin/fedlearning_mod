{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from utils.customloader import CustomDataset, DatasetSplit\n",
    "from utils.separate_into_classes import separate_into_classes\n",
    "#from utils.arguments import Args\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar\n",
    "from models.Fed import FedAvg\n",
    "from models.test import test_img\n",
    "from utils.smooth_crossentropy import SmoothCrossEntropyLoss\n",
    "from utils.dataloader import get_dataloader, set_seed\n",
    "from utils.train_glob import train_global_model, test_model\n",
    "import random\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "class Args:\n",
    "    #federated arugments\n",
    "    epochs=10\n",
    "    num_users=10\n",
    "    local_ep=3\n",
    "    local_bs=10\n",
    "    bs=128\n",
    "    lr=0.01\n",
    "    momentum=0.5\n",
    "    split='user'\n",
    "    \n",
    "    \n",
    "    #model arguments\n",
    "    model='mnist'\n",
    "    kernel_num=9\n",
    "    kernel_sizes='3,4,5'\n",
    "    norm='batch_norm'\n",
    "    num_filters=32\n",
    "    max_pool=True\n",
    "    \n",
    "    #other arguments\n",
    "    #data='mnist'\n",
    "    #iid='store_true'\n",
    "    num_channels=1\n",
    "    num_classes=10\n",
    "    #stopping_rounds=10\n",
    "    verbose='store_true'\n",
    "    seed=1\n",
    "    #all_clients='store_true'\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    \n",
    "\n",
    "args = Args()    \n",
    "##############SET SEEDS FOR REPRODUCIBILITY#############\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "# if you are suing GPU\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True    \n",
    "##############~SET SEEDS FOR REPRODUCIBILITY#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataloader /  Model / Optimizer / Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ transforms.ToTensor(),  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "global_train_loader = DataLoader(dataset_train, batch_size=args.bs, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(dataset_test, batch_size=args.bs, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before fedlearning\n",
    "path_checkpoint='./test'\n",
    "\n",
    "net_glob = CNNMnist(args=args).to(args.device)\n",
    "net_glob.train()\n",
    "sloss = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Distribution of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_users = mnist_iid(global_train_loader.dataset.data, args.num_users)\n",
    "len(dict_users[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(global_train_loader.dataset.targets, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)\n",
    "sorted_y = copy.deepcopy(global_train_loader.dataset.targets)\n",
    "sorted_index_y = np.argsort(np.squeeze(sorted_y))\n",
    "\n",
    "class_dist=[]\n",
    "\n",
    "for i in range(args.num_classes):\n",
    "    print(i)\n",
    "    class_dist.append(np.array(sorted_index_y[sum(counts[:i]):sum(counts[:i+1])], dtype=np.int64))\n",
    "    \n",
    "non_iid = np.array(class_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual = []\n",
    "for j in range(10):\n",
    "    individual.append(np.array_split(class_dist[j], 10))\n",
    "\n",
    "user_dist=[]\n",
    "for i in range(10):\n",
    "    temp=[]\n",
    "    for j in range(10):\n",
    "        temp.append(individual[j][i])\n",
    "        \n",
    "    \n",
    "    user_dist.append((np.concatenate(temp)).astype(np.int64))    \n",
    "    \n",
    "iid=np.array(user_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_noniid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_shards, num_imgs = 200, 300\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    labels = dataset.train_labels.numpy()\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()]\n",
    "    idxs = idxs_labels[0,:]\n",
    "\n",
    "    # divide and assign\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate((dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imtl/.conda/envs/imtl/lib/python3.8/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "non_iid_again = mnist_noniid(global_train_loader.dataset, args.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_data_set = non_iid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.multiprocessing import work, multi_train_local_dif\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchvision import datasets, transforms\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Run FedLearning in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select percentage of participating clients\n",
    "frac=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sloss2 = F.cross_entropy\n",
    "global_model = copy.deepcopy(net_glob)\n",
    "\n",
    "for i in range(args.epochs):\n",
    "\n",
    "    m = max(int(frac * args.num_users), 1)\n",
    "\n",
    "    print('--------------------------------------------')\n",
    "    print(\"\\n\\n\\nstart training epoch : \" + str(i) + \"\\nNum_users:  \" + str(m) + \"\\n\\n\\n\")\n",
    "\n",
    "    print('--------------------------------------------')\n",
    "\n",
    "    procs=[]\n",
    "    loss_locals=[]\n",
    "    w_locals=[]        \n",
    "\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "    for idx in idxs_users:\n",
    "\n",
    "        local = LocalUpdate(args=args, user_num=idx, loss_func=sloss, dataset=global_train_loader.dataset, idxs=cur_data_set[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(global_model).to(args.device))\n",
    "\n",
    "        w_locals.append(w)\n",
    "        loss_locals.append(loss)\n",
    "\n",
    "    print('--------------------------------------------\\n\\n')\n",
    "    w_glob = FedAvg(w_locals)\n",
    "    global_model.load_state_dict(w_glob)\n",
    "    test_model(global_model, test_loader, sloss)\n",
    "    print('\\n\\n--------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Clustering Learning -- checkpoint\n",
      "\n",
      "Test set: Average loss: 0.01570 \n",
      "Accuracy: 3394/10000 (33.94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#After fedlearning\n",
    "print('Before Clustering Learning -- checkpoint')\n",
    "test_model(global_model, test_loader, sloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config user.name \"sonhamin\"\n",
    "!git config user.email \"sonhamin3@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git remote add origin https://github.com/sonhamin/fedlearning_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add ./FedLearning.ipynb\n",
    "!git add ./utils/*\n",
    "!git add ./models/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master (root-commit) 5ccb8c7] add files\r\n",
      " 9 files changed, 783 insertions(+)\r\n",
      " create mode 100644 FedLearning.ipynb\r\n",
      " create mode 100644 models/Fed.py\r\n",
      " create mode 100644 models/Update.py\r\n",
      " create mode 100644 models/__init__.py\r\n",
      " create mode 100644 utils/__init__.py\r\n",
      " create mode 100644 utils/customloader.py\r\n",
      " create mode 100644 utils/dataloader.py\r\n",
      " create mode 100644 utils/multiprocessing.py\r\n",
      " create mode 100644 utils/train_glob.py\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"add files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username for 'https://github.com': ^C\n"
     ]
    }
   ],
   "source": [
    "!git push -u origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FedLearning.ipynb  data  models  utils\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
